{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 128\n",
    "data_path='/data/mnist'\n",
    "# I have an M2 macbook, so I can only use CPU\n",
    "device = torch.device(\"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to transform each MNIST image into something that can serve as input into our SNN\n",
    "Spiking neural networks work with 0's and 1's, so we want each image to be in a format the we can work with.\n",
    "1. We standardize each PIL image to 28 x 28, grey scaled tensors where each value is normalized to 0 and 1\n",
    "2. Then we gather our train and test data and\n",
    "3. Instantiate our data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "beta = 0.95\n",
    "\n",
    "# Define Network\n",
    "class SNN_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "net = SNN_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0\n",
      "Train Set Loss: 62.36\n",
      "Test Set Loss: 53.44\n",
      "Train set accuracy for a single minibatch: 37.50%\n",
      "Test set accuracy for a single minibatch: 23.44%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 50\n",
      "Train Set Loss: 16.69\n",
      "Test Set Loss: 10.59\n",
      "Train set accuracy for a single minibatch: 89.06%\n",
      "Test set accuracy for a single minibatch: 92.19%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 100\n",
      "Train Set Loss: 12.56\n",
      "Test Set Loss: 9.61\n",
      "Train set accuracy for a single minibatch: 88.28%\n",
      "Test set accuracy for a single minibatch: 89.84%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 150\n",
      "Train Set Loss: 11.23\n",
      "Test Set Loss: 6.80\n",
      "Train set accuracy for a single minibatch: 92.97%\n",
      "Test set accuracy for a single minibatch: 94.53%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 200\n",
      "Train Set Loss: 11.86\n",
      "Test Set Loss: 8.07\n",
      "Train set accuracy for a single minibatch: 90.62%\n",
      "Test set accuracy for a single minibatch: 89.06%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 250\n",
      "Train Set Loss: 8.54\n",
      "Test Set Loss: 13.97\n",
      "Train set accuracy for a single minibatch: 85.16%\n",
      "Test set accuracy for a single minibatch: 86.72%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 300\n",
      "Train Set Loss: 5.31\n",
      "Test Set Loss: 9.40\n",
      "Train set accuracy for a single minibatch: 96.09%\n",
      "Test set accuracy for a single minibatch: 94.53%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 350\n",
      "Train Set Loss: 7.12\n",
      "Test Set Loss: 12.30\n",
      "Train set accuracy for a single minibatch: 95.31%\n",
      "Test set accuracy for a single minibatch: 91.41%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 400\n",
      "Train Set Loss: 7.55\n",
      "Test Set Loss: 5.06\n",
      "Train set accuracy for a single minibatch: 93.75%\n",
      "Test set accuracy for a single minibatch: 93.75%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 450\n",
      "Train Set Loss: 7.32\n",
      "Test Set Loss: 6.41\n",
      "Train set accuracy for a single minibatch: 92.19%\n",
      "Test set accuracy for a single minibatch: 92.19%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 08:57:44) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7288f315087fdb0a15835a979a50c8db3e0e21492381bafafe9d84f995bbb7dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
